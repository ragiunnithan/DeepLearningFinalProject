{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Classification of Acute Lymphocytic Leukemia (ALL) with Convolutional Neural Network\n\nImporting all the libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-26T17:07:27.681877Z","iopub.execute_input":"2021-11-26T17:07:27.682222Z","iopub.status.idle":"2021-11-26T17:07:27.704148Z","shell.execute_reply.started":"2021-11-26T17:07:27.682135Z","shell.execute_reply":"2021-11-26T17:07:27.702942Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport imageio\nimport numpy as np \nfrom timeit import default_timer as timer\nfrom tensorflow.keras.preprocessing import image \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import models, layers","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:14.731916Z","iopub.execute_input":"2021-11-26T19:13:14.732221Z","iopub.status.idle":"2021-11-26T19:13:20.077776Z","shell.execute_reply.started":"2021-11-26T19:13:14.732142Z","shell.execute_reply":"2021-11-26T19:13:20.077000Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Getting the file paths for the data. The folder \"ALL\" has the data for Acute Lymphoblastic Leukemia (ALL) and the folder \"hem\" has data for the Normal. Currently we are loading only training and validation.","metadata":{}},{"cell_type":"code","source":"fold0_all = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_0/all\"\nfold1_all = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_1/all\"\nfold2_all = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_2/all\"\n\n#hem is the normal one\nfold0_nrml = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_0/hem\"\nfold1_nrml = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_1/hem\"\nfold2_nrml = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/training_data/fold_2/hem\"\n\nval_data = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data/\"\nval_labels = pd.read_csv(\"/kaggle/input/leukemia-classification/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data_labels.csv\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.079520Z","iopub.execute_input":"2021-11-26T19:13:20.079812Z","iopub.status.idle":"2021-11-26T19:13:20.105869Z","shell.execute_reply.started":"2021-11-26T19:13:20.079774Z","shell.execute_reply":"2021-11-26T19:13:20.105095Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Reading and plotting one image from the Cancer data (ALL) and Normal data.\n\n### Cancer Cell","metadata":{}},{"cell_type":"code","source":"can_image_fnames = os.listdir(fold0_all)\n\n##Plotting a single image\ncancer_img = imageio.imread(os.path.join(fold0_all,\n                                         can_image_fnames[5]))\nplt.imshow(cancer_img)\nplt.title('Cancer - ALL Cell')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.107792Z","iopub.execute_input":"2021-11-26T19:13:20.108024Z","iopub.status.idle":"2021-11-26T19:13:20.522969Z","shell.execute_reply.started":"2021-11-26T19:13:20.107990Z","shell.execute_reply":"2021-11-26T19:13:20.522267Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### Normal Cell","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.524314Z","iopub.execute_input":"2021-11-26T19:13:20.524863Z","iopub.status.idle":"2021-11-26T19:13:20.528822Z","shell.execute_reply.started":"2021-11-26T19:13:20.524823Z","shell.execute_reply":"2021-11-26T19:13:20.528112Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#blood cell without cancer - normal cells\nnrml_image_fnames = os.listdir(fold0_nrml)\nnrml_img = imageio.imread(os.path.join(fold0_nrml,\n                                         nrml_image_fnames[5]))\n\nplt.imshow(nrml_img)\nplt.title('Normal - Hem Cell')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.531327Z","iopub.execute_input":"2021-11-26T19:13:20.531793Z","iopub.status.idle":"2021-11-26T19:13:20.904947Z","shell.execute_reply.started":"2021-11-26T19:13:20.531758Z","shell.execute_reply":"2021-11-26T19:13:20.904180Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The image is a 450 * 450 pixels with colors 'RGB'","metadata":{}},{"cell_type":"code","source":"nrml_img.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.906519Z","iopub.execute_input":"2021-11-26T19:13:20.907108Z","iopub.status.idle":"2021-11-26T19:13:20.914771Z","shell.execute_reply.started":"2021-11-26T19:13:20.906962Z","shell.execute_reply":"2021-11-26T19:13:20.913875Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Read all the normal and cancer images as a list. Create a dataframe with file path as one column and the cancer type as another columns","metadata":{}},{"cell_type":"code","source":"def get_path_image(folder):\n    image_paths = []\n    image_fnames = os.listdir(folder) \n    for img_id in range(len(image_fnames)):\n        img = os.path.join(folder,image_fnames[img_id])\n        image_paths.append(img)\n    \n    return image_paths","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.918109Z","iopub.execute_input":"2021-11-26T19:13:20.918339Z","iopub.status.idle":"2021-11-26T19:13:20.923913Z","shell.execute_reply.started":"2021-11-26T19:13:20.918311Z","shell.execute_reply":"2021-11-26T19:13:20.923235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The number of cancer images in the train dataset is:","metadata":{}},{"cell_type":"code","source":"#image absolute paths for cancer cells and normal cells\ncancer_lst = []\n\nfor i in [fold0_all,fold1_all,fold2_all]:\n    paths = get_path_image(i)\n    cancer_lst.extend(paths)\n    \n    \nprint(len(cancer_lst))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:20.925441Z","iopub.execute_input":"2021-11-26T19:13:20.926390Z","iopub.status.idle":"2021-11-26T19:13:21.206729Z","shell.execute_reply.started":"2021-11-26T19:13:20.926346Z","shell.execute_reply":"2021-11-26T19:13:21.205915Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The number of normal images in the train dataset.","metadata":{}},{"cell_type":"code","source":"normal_lst = []\nfor i in [fold0_nrml,fold1_nrml,fold2_nrml]:\n    paths = get_path_image(i)\n    normal_lst.extend(paths)\n   \n   \nprint(len(normal_lst))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:21.208059Z","iopub.execute_input":"2021-11-26T19:13:21.208426Z","iopub.status.idle":"2021-11-26T19:13:21.426018Z","shell.execute_reply.started":"2021-11-26T19:13:21.208385Z","shell.execute_reply":"2021-11-26T19:13:21.425142Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"cancer_dict = {\"x_col\":cancer_lst,\n          \"y_col\":[np.nan for x in range(len(cancer_lst))]}\n\n\ncancer_dict[\"y_col\"] = \"ALL\"\n\nnormal_dict = {\"x_col\":normal_lst,\n          \"y_col\":[np.nan for x in range(len(normal_lst))]}\n\n\nnormal_dict[\"y_col\"] = \"HEM\"\n\ncancer_df = pd.DataFrame(cancer_dict)\nnormal_df = pd.DataFrame(normal_dict)\n\ntrain_df = cancer_df.append(normal_df, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:21.427251Z","iopub.execute_input":"2021-11-26T19:13:21.427963Z","iopub.status.idle":"2021-11-26T19:13:21.439438Z","shell.execute_reply.started":"2021-11-26T19:13:21.427921Z","shell.execute_reply":"2021-11-26T19:13:21.438545Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"###  Bar chart for showing the number of images for each cell type.","metadata":{}},{"cell_type":"code","source":"\nplt.bar(['Normal', 'ALL'], [len(normal_lst), len(cancer_lst)])\nplt.title('Bar chart for showing the percentage of images in each cell type')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:21.441220Z","iopub.execute_input":"2021-11-26T19:13:21.441538Z","iopub.status.idle":"2021-11-26T19:13:21.617246Z","shell.execute_reply.started":"2021-11-26T19:13:21.441480Z","shell.execute_reply":"2021-11-26T19:13:21.616465Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Pie chart showing the percentage of each cell type","metadata":{}},{"cell_type":"code","source":"plt.pie([len(train_df[train_df[\"y_col\"]==\"ALL\"]),len(train_df[train_df[\"y_col\"]==\"HEM\"])],\n        labels=[\"ALL\",\"Normal\"],autopct='%.f'\n       )\nplt.title('Pie Chart for percentage of each cell type')\nplt.gca()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:21.618867Z","iopub.execute_input":"2021-11-26T19:13:21.619392Z","iopub.status.idle":"2021-11-26T19:13:21.717302Z","shell.execute_reply.started":"2021-11-26T19:13:21.619350Z","shell.execute_reply":"2021-11-26T19:13:21.716656Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"select_normal = np.random.choice(normal_lst, 3, replace = False)\nselect_all = np.random.choice(cancer_lst, 3, replace = False)\n\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in range(6):\n    if i < 3:\n        fp = select_normal[i]\n        label = 'Normal'\n    else:\n        fp = select_all[i-3]\n        label = 'ALL'\n    ax = fig.add_subplot(2, 3, i+1)\n    fn = image.load_img(fp, target_size = (100,100),\n                        color_mode='rgb')\n    plt.imshow(fn, cmap='Greys_r')\n    plt.title(label)\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:21.718510Z","iopub.execute_input":"2021-11-26T19:13:21.718905Z","iopub.status.idle":"2021-11-26T19:13:22.277458Z","shell.execute_reply.started":"2021-11-26T19:13:21.718869Z","shell.execute_reply":"2021-11-26T19:13:22.276781Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Normal cells are mostly sperical and non-clefted cells with homogeneous chromatin and few vacuoles. ALL cells are irregularly shaped, clefted cells with hetrogeneous chromatin and multiple nucleoli and vacuoles.\n\n#### Average Images\nLet us average 50 cells of each sub class and see how much variation is there to identify meaningful differences.","metadata":{}},{"cell_type":"code","source":"#function for processing images to numpy array for creating mean\ndef img2np(fn_list, size = (32, 32)):\n   \n    i = 0\n    for fp in fn_list:\n        \n        current_image = image.load_img(fp, \n                                       target_size = size, \n                                       color_mode = 'grayscale')\n        \n        img_ts = image.img_to_array(current_image)\n        img_ts = [img_ts.ravel()]\n             \n        \n        \n        \n        if i == 0:\n            full_mat = img_ts\n            \n        else: \n            full_mat = np.concatenate((full_mat, img_ts))    \n        i = i + 1    \n    return full_mat\n\n\n\nnormal_npArray = img2np(normal_lst)\ncancer_npArray = img2np(cancer_lst)\n\n\n##Let us select 50 images to make the comparison clear\n\n\nnrml_images_sub = normal_npArray[:50]\ncnr_images_sub = cancer_npArray[:50]\n\n\ndef plt_mean(mat , title, size = (32, 32)):\n    \n    mean_img = np.mean(mat , axis = 0)\n    mean_img = mean_img.reshape(size)\n    plt.imshow(mean_img, vmin=0, vmax=255)\n    plt.title(f'Average {title}')\n    plt.axis('off')\n    plt.show()\n    return mean_img\n\nnrml_mean = plt_mean(nrml_images_sub , \"Normal\")\ncnr_mean = plt_mean(cnr_images_sub, \"Cancer - ALL\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:13:22.280206Z","iopub.execute_input":"2021-11-26T19:13:22.280895Z","iopub.status.idle":"2021-11-26T19:15:22.149654Z","shell.execute_reply.started":"2021-11-26T19:13:22.280855Z","shell.execute_reply":"2021-11-26T19:15:22.148815Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The mean images can't detect much diferences between the classes. Though the ALL cells are bit larger than normal cell.","metadata":{}},{"cell_type":"markdown","source":"Reading the test and validation datset file locations and converting them to a dataframe","metadata":{}},{"cell_type":"code","source":"#Reading the validation data\nvalidation_list = get_path_image(val_data)\n\n\n##Convert the list to a dictionary. The labels are stored in the val_labels\n#3So we create a dictionary with x as the file name and y as the labels\n##The labels are having 0's and 1's.\n##0 means normal and 1 means cancer - ALL\n\nvalidation_dict = {\"x_col\":validation_list,\n                   \"y_col\":val_labels[\"labels\"]}\n\nvalidation_df = pd.DataFrame(validation_dict)\n\nvalidation_df[\"y_col\"].replace(to_replace = [1,0], value = [\"ALL\",\"HEM\"], inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:22.154502Z","iopub.execute_input":"2021-11-26T19:15:22.156772Z","iopub.status.idle":"2021-11-26T19:15:22.772191Z","shell.execute_reply.started":"2021-11-26T19:15:22.156725Z","shell.execute_reply":"2021-11-26T19:15:22.771474Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Reading the test data set","metadata":{}},{"cell_type":"code","source":"test_data = \"/kaggle/input/leukemia-classification/C-NMC_Leukemia/testing_data/C-NMC_test_final_phase_data/\"\ntest_list = get_path_image(test_data)\n\ntest_dict = {\"x_col\":test_list}\n\ntest_df = pd.DataFrame(test_dict)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:22.774406Z","iopub.execute_input":"2021-11-26T19:15:22.774868Z","iopub.status.idle":"2021-11-26T19:15:23.351416Z","shell.execute_reply.started":"2021-11-26T19:15:22.774832Z","shell.execute_reply":"2021-11-26T19:15:23.350694Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nWith keras, image preprocessing has become much easier. Instead of reading subfolders for all files, load pictures and converting it to numpy arrays\nKeras provides API calls. flow_from_dataframe allows us to input a pandas dataframe which contains the filenames, with or without extension, as one column and\nand a column which has the class names and directly read the images from the directory with their respective class names mapped.\n","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale=1./255 #pixel values are 255 maximum\n         )\n\n\ntest_datagen = ImageDataGenerator(\n        rescale=1./255 )\n\ntrain_generator = train_datagen.flow_from_dataframe(\n                  train_df,\n                  x_col = \"x_col\",\n                  y_col = \"y_col\",\n                  target_size = (256, 256),\n                 \n                  #batch_size = 32,\n                  color_mode = \"rgb\",\n                  shuffle = True,\n                  class_mode = \"binary\")\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n                  validation_df,\n                  x_col = \"x_col\",\n                  y_col = \"y_col\",\n                  target_size = (256, 256),                  \n                  #batch_size = 32,\n                  color_mode = \"rgb\",\n                  shuffle = True,\n                  class_mode = \"binary\")\n\ntest_generator = test_datagen.flow_from_dataframe(\n                  test_df,\n                  x_col = \"x_col\",\n                  target_size = (256, 256),\n                  color_mode = \"rgb\",\n                  class_mode = None,\n                  shuffle = True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:23.352851Z","iopub.execute_input":"2021-11-26T19:15:23.353117Z","iopub.status.idle":"2021-11-26T19:15:33.844032Z","shell.execute_reply.started":"2021-11-26T19:15:23.353081Z","shell.execute_reply":"2021-11-26T19:15:33.843181Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"\n\n### A Model with one convolution and one dense layer\nConvolutional layer uses fewer parameters by forcing input values to share the parameters.\nDense layer uses a linear operation, meaning, every output is formed by the function based on every input.\nIn other words, every input is forced into the function, and then the \"Neural Network\" learns it's relation to the output. \nThere will be n*m connections, where n denotes the number of inputs and m denotes the number of outputs.\n\nThe output of the convolutional layer is formed by just a small size of inputs which depends on the filter's size and the weights are shared \nfor all pixels. The output is constructed by using the same co-efficient for all the pixels by using the neighbouring pixels as inputs.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"model1 = models.Sequential()\nmodel1.add(layers.Conv2D(64, 3, activation = 'relu', input_shape = (256, 256, 3)))\n\nmodel1.add(layers.MaxPooling2D((2, 2)))\nmodel1.add(layers.Flatten())\nmodel1.add(layers.Dense(512, activation = 'relu'))\nmodel1.add(layers.Dense(1, activation='sigmoid'))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:33.845344Z","iopub.execute_input":"2021-11-26T19:15:33.848283Z","iopub.status.idle":"2021-11-26T19:15:36.405365Z","shell.execute_reply.started":"2021-11-26T19:15:33.848252Z","shell.execute_reply":"2021-11-26T19:15:36.404629Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n# compiling models\nmodel1.compile(loss='binary_crossentropy',\n              optimizer= 'adam',\n              metrics=['accuracy', 'Recall'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:36.406712Z","iopub.execute_input":"2021-11-26T19:15:36.406962Z","iopub.status.idle":"2021-11-26T19:15:36.418574Z","shell.execute_reply.started":"2021-11-26T19:15:36.406928Z","shell.execute_reply":"2021-11-26T19:15:36.417797Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"start = timer()\n\nhistory = model1.fit(train_generator , \n                    epochs=5, \n                    validation_data=validation_generator, \n                    workers = 7\n                   )\n\nend = timer()\nelapsed = end - start\nprint('Total Time Elapsed: ', int(elapsed//60), ' minutes ', (round(elapsed%60)), ' seconds')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:15:36.419710Z","iopub.execute_input":"2021-11-26T19:15:36.420363Z","iopub.status.idle":"2021-11-26T19:18:47.190529Z","shell.execute_reply.started":"2021-11-26T19:15:36.420324Z","shell.execute_reply":"2021-11-26T19:18:47.189681Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"scores = model1.evaluate(test_generator, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:18:47.192046Z","iopub.execute_input":"2021-11-26T19:18:47.192469Z","iopub.status.idle":"2021-11-26T19:19:31.283409Z","shell.execute_reply.started":"2021-11-26T19:18:47.192431Z","shell.execute_reply":"2021-11-26T19:19:31.282727Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\nscores = model1.evaluate(test_generator, verbose=1)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T19:19:31.284985Z","iopub.execute_input":"2021-11-26T19:19:31.285265Z","iopub.status.idle":"2021-11-26T19:19:40.330301Z","shell.execute_reply.started":"2021-11-26T19:19:31.285228Z","shell.execute_reply":"2021-11-26T19:19:40.329612Z"},"trusted":true},"execution_count":22,"outputs":[]}]}